# 最终代码



---

```
import tensorflow as tf
#输入数据
from tensorflow.examples.tutorials.mnist import input_data
#Tensorflow 自带，用来下载并返回mnist数据。
mnist = input_data.read_data_sets("/tmp/data", one_hot=True)

#定义网络的超参数
learning_rate = 0.001
training_iters = 200000
batch_size = 64
display_step = 20

#定义网络参数
n_input = 784 #输入的维度
n_classes = 10 #标记的维度
dropout = 0.75 #Dropout的概率

#输入占位符
x = tf.placeholder(tf.float32, [None, n_input])
y = tf.placeholder(tf.float32, [None, n_classes])
keep_prob = tf.placeholder(tf.float32)


#构建网络模型

#定义卷积操作
def conv2d(name,x, w, b, strides=1):
    x = tf.nn.conv2d(x, w, strides=[1, strides, strides, 1], padding='SAME')
    x = tf.nn.bias_add(x, b)
    return tf.nn.relu(x,name=name) #使用relu激活函数

#定义池化层操作
def maxpool2d(name,x, k=2):
    return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1], padding='SAME',name=name) #最大值池化

#规范化操作
def norm(name, l_input, lsize=4):
    return tf.nn.lrn(l_input, lsize, bias=1.0, alpha=0.001 / 9.0,beta=0.75, name=name)

#定义所有的网络参数
weights = {
    'wc1': tf.Variable(tf.random_normal([11, 11, 1, 96])),
    'wc2': tf.Variable(tf.random_normal([5, 5, 96, 256])),
    'wc3': tf.Variable(tf.random_normal([3, 3, 256, 384])),
    'wc4': tf.Variable(tf.random_normal([3, 3, 384, 384])),
    'wc5': tf.Variable(tf.random_normal([3, 3, 384, 256])),
    'wd1': tf.Variable(tf.random_normal([4*4*256, 4096])),
    'wd2': tf.Variable(tf.random_normal([4096, 1024])),
    'out': tf.Variable(tf.random_normal([1024, n_classes]))
}
biases = {
    'bc1': tf.Variable(tf.random_normal([96])),
    'bc2': tf.Variable(tf.random_normal([256])),
    'bc3': tf.Variable(tf.random_normal([384])),
    'bc4': tf.Variable(tf.random_normal([384])),
    'bc5': tf.Variable(tf.random_normal([256])),
    'bd1': tf.Variable(tf.random_normal([4096])),
    'bd2': tf.Variable(tf.random_normal([1024])),
    'out': tf.Variable(tf.random_normal([n_classes]))
}


#定义AlexNet网络M模型
def alex_net(x, weights, biases, dropout):
    #向量转为矩阵
    x = tf.reshape(x, shape=[-1, 28, 28, 1])
    
    #第一层卷积
    conv1 = conv2d('conv1', x, weights['wc1'], biases['bc1'])
    #下采样
    pool1 = maxpool2d('pool1', conv1, k=2)
    #规范化
    norm1 = norm('norm1', pool1, lsize=4)
    
    #第二层卷积
    conv2 = conv2d('conv2', norm1, weights['wc2'], biases['bc2'])
    #最大池化
    pool2 = maxpool2d('pool2', conv2, k=2)
    #规范化
    norm2 = norm('norm', pool2, lsize=4)
    
    #第三层卷积
    conv3 = conv2d('conv3', norm2, weights['wc3'], biases['bc3'])
    #规范化
    norm3 = norm('norm3', conv3, lsize=4)
    
    #第四层卷积
    conv4 = conv2d('conv4', norm3, weights['wc4'], biases['bc4'])
    
    #第五层卷积
    conv5 = conv2d('conv5', conv4, weights['wc5'], biases['bc5'])
    #最大池化
    pool5 = maxpool2d('pool5', conv5, k=2)
    #规范化
    norm5 = norm('norm5', pool5, lsize=4)
    
    
    #全连接层1
    fc1 = tf.reshape(norm5, [-1, weights['wd1'].get_shape().as_list()[0]])
    fc1 = tf.add(tf.matmul(fc1, weights['wd1']),biases['bd1'])
    fc1 = tf.nn.relu(fc1)
    #dropout
    fc1 = tf.nn.dropout(fc1,dropout)
    
    #全连接层2
    fc2 = tf.reshape(fc1, [-1, weights['wd2'].get_shape().as_list()[0]])
    fc2 = tf.add(tf.matmul(fc2, weights['wd2']),biases['bd2'])
    fc2 = tf.nn.relu(fc2)
    #dropout
    fc2 = tf.nn.dropout(fc2,dropout)
    
    #输出层
    out = tf.add(tf.matmul(fc2, weights['out']) ,biases['out'])
    return out


#构建模型，定义损失函数和优化器，并构建评估函数

#构建模型
pred = alex_net(x, weights, biases, keep_prob)

#定义损失函数和优化器
cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))
optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)

#评估函数
correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))
accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))


#训练模型和评估模型

#初始化常量
init = tf.global_variables_initializer()

#开启一个训练
with tf.Session() as sess:
    sess.run(init)
    step = 1
    #开始训练
    while step * batch_size < training_iters:
        batch_x, batch_y = mnist.train.next_batch(batch_size)
        sess.run(optimizer, feed_dict={x: batch_x, y: batch_y, keep_prob: dropout})
        if step % display_step == 0:
            #计算损失值和准确度， 输出
            loss,acc = sess.run([cost,accuracy], feed_dict={x: batch_x, y:batch_y, keep_prob: 1.})
            print("Iter " + str(step*batch_size) + ", Minibatch Loss= " + "{:.6f}".format(loss) + ", Training Accuracy=" + "{:.5f}".format(acc))
        step += 1
    print("Optimization Finished!")
    #计算测试集的精确度
    print("Testing Accuracy:",
         sess.run(accuracy, feed_dict={x: mnist.test.images[:256], y: mnist.test.labels[:256], keep_prob: 1.}))
```

```
WARNING:tensorflow:From <ipython-input-1-46d8906324e6>:5: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.
Instructions for updating:
Please use alternatives such as official/mnist/dataset.py from tensorflow/models.
WARNING:tensorflow:From D:\Anaconda\envs\tensorflow_gpu\lib\site-packages\tensorflow\contrib\learn\python\learn\datasets\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.
Instructions for updating:
Please write your own downloading logic.
WARNING:tensorflow:From D:\Anaconda\envs\tensorflow_gpu\lib\site-packages\tensorflow\contrib\learn\python\learn\datasets\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.
Instructions for updating:
Please use tf.data to implement this functionality.
Extracting /tmp/data\train-images-idx3-ubyte.gz
WARNING:tensorflow:From D:\Anaconda\envs\tensorflow_gpu\lib\site-packages\tensorflow\contrib\learn\python\learn\datasets\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.
Instructions for updating:
Please use tf.data to implement this functionality.
Extracting /tmp/data\train-labels-idx1-ubyte.gz
WARNING:tensorflow:From D:\Anaconda\envs\tensorflow_gpu\lib\site-packages\tensorflow\contrib\learn\python\learn\datasets\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.
Instructions for updating:
Please use tf.one_hot on tensors.
Extracting /tmp/data\t10k-images-idx3-ubyte.gz
Extracting /tmp/data\t10k-labels-idx1-ubyte.gz
WARNING:tensorflow:From D:\Anaconda\envs\tensorflow_gpu\lib\site-packages\tensorflow\contrib\learn\python\learn\datasets\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.
Instructions for updating:
Please use alternatives such as official/mnist/dataset.py from tensorflow/models.
WARNING:tensorflow:From <ipython-input-1-46d8906324e6>:123: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.
Instructions for updating:

Future major versions of TensorFlow will allow gradients to flow
into the labels input on backprop by default.

See `tf.nn.softmax_cross_entropy_with_logits_v2`.

Iter 1280, Minibatch Loss= 26640.154297, Training Accuracy=0.59375
Iter 2560, Minibatch Loss= 12629.213867, Training Accuracy=0.68750
Iter 3840, Minibatch Loss= 12491.900391, Training Accuracy=0.65625
Iter 5120, Minibatch Loss= 2686.045410, Training Accuracy=0.87500
Iter 6400, Minibatch Loss= 3591.543213, Training Accuracy=0.78125
Iter 7680, Minibatch Loss= 5779.962891, Training Accuracy=0.79688
Iter 8960, Minibatch Loss= 3235.060059, Training Accuracy=0.84375
Iter 10240, Minibatch Loss= 138.683777, Training Accuracy=0.93750
Iter 11520, Minibatch Loss= 4092.701904, Training Accuracy=0.85938
Iter 12800, Minibatch Loss= 1488.665283, Training Accuracy=0.89062
Iter 14080, Minibatch Loss= 705.744751, Training Accuracy=0.92188
Iter 15360, Minibatch Loss= 2116.020508, Training Accuracy=0.87500
Iter 16640, Minibatch Loss= 1414.261230, Training Accuracy=0.90625
Iter 17920, Minibatch Loss= 4038.590576, Training Accuracy=0.85938
Iter 19200, Minibatch Loss= 1327.266113, Training Accuracy=0.95312
Iter 20480, Minibatch Loss= 260.442474, Training Accuracy=0.95312
Iter 21760, Minibatch Loss= 667.554810, Training Accuracy=0.92188
Iter 23040, Minibatch Loss= 1682.138672, Training Accuracy=0.90625
Iter 24320, Minibatch Loss= 1274.328857, Training Accuracy=0.90625
Iter 25600, Minibatch Loss= 2629.869385, Training Accuracy=0.90625
Iter 26880, Minibatch Loss= 902.850342, Training Accuracy=0.93750
Iter 28160, Minibatch Loss= 1441.787231, Training Accuracy=0.87500
Iter 29440, Minibatch Loss= 985.744019, Training Accuracy=0.92188
Iter 30720, Minibatch Loss= 670.368530, Training Accuracy=0.89062
Iter 32000, Minibatch Loss= 352.881378, Training Accuracy=0.95312
Iter 33280, Minibatch Loss= 936.911621, Training Accuracy=0.93750
Iter 34560, Minibatch Loss= 584.226929, Training Accuracy=0.95312
Iter 35840, Minibatch Loss= 644.337646, Training Accuracy=0.93750
Iter 37120, Minibatch Loss= 1278.375488, Training Accuracy=0.87500
Iter 38400, Minibatch Loss= 864.174011, Training Accuracy=0.90625
Iter 39680, Minibatch Loss= 453.956543, Training Accuracy=0.93750
Iter 40960, Minibatch Loss= 535.692871, Training Accuracy=0.92188
Iter 42240, Minibatch Loss= 465.206360, Training Accuracy=0.92188
Iter 43520, Minibatch Loss= 550.550415, Training Accuracy=0.92188
Iter 44800, Minibatch Loss= 735.001709, Training Accuracy=0.93750
Iter 46080, Minibatch Loss= 900.309082, Training Accuracy=0.93750
Iter 47360, Minibatch Loss= 447.194702, Training Accuracy=0.89062
Iter 48640, Minibatch Loss= 424.719788, Training Accuracy=0.93750
Iter 49920, Minibatch Loss= 376.308899, Training Accuracy=0.92188
Iter 51200, Minibatch Loss= 684.810974, Training Accuracy=0.89062
Iter 52480, Minibatch Loss= 456.968109, Training Accuracy=0.95312
Iter 53760, Minibatch Loss= 922.018616, Training Accuracy=0.92188
Iter 55040, Minibatch Loss= 36.835999, Training Accuracy=0.98438
Iter 56320, Minibatch Loss= 621.169800, Training Accuracy=0.90625
Iter 57600, Minibatch Loss= 377.766174, Training Accuracy=0.95312
Iter 58880, Minibatch Loss= 523.538513, Training Accuracy=0.92188
Iter 60160, Minibatch Loss= 0.000000, Training Accuracy=1.00000
Iter 61440, Minibatch Loss= 295.283966, Training Accuracy=0.95312
Iter 62720, Minibatch Loss= 130.590729, Training Accuracy=0.96875
Iter 64000, Minibatch Loss= 482.239441, Training Accuracy=0.92188
Iter 65280, Minibatch Loss= 650.396484, Training Accuracy=0.90625
Iter 66560, Minibatch Loss= 162.108749, Training Accuracy=0.95312
Iter 67840, Minibatch Loss= 559.215637, Training Accuracy=0.95312
Iter 69120, Minibatch Loss= 507.166931, Training Accuracy=0.93750
Iter 70400, Minibatch Loss= 242.806885, Training Accuracy=0.95312
Iter 71680, Minibatch Loss= 98.458313, Training Accuracy=0.96875
Iter 72960, Minibatch Loss= 180.905045, Training Accuracy=0.96875
Iter 74240, Minibatch Loss= 1504.601318, Training Accuracy=0.87500
Iter 75520, Minibatch Loss= 468.142303, Training Accuracy=0.90625
Iter 76800, Minibatch Loss= 377.393463, Training Accuracy=0.93750
Iter 78080, Minibatch Loss= 153.621017, Training Accuracy=0.95312
Iter 79360, Minibatch Loss= 175.226776, Training Accuracy=0.93750
Iter 80640, Minibatch Loss= 478.241638, Training Accuracy=0.90625
Iter 81920, Minibatch Loss= 68.366531, Training Accuracy=0.95312
Iter 83200, Minibatch Loss= 95.447876, Training Accuracy=0.96875
Iter 84480, Minibatch Loss= 247.514862, Training Accuracy=0.93750
Iter 85760, Minibatch Loss= 69.913910, Training Accuracy=0.98438
Iter 87040, Minibatch Loss= 177.342697, Training Accuracy=0.93750
Iter 88320, Minibatch Loss= 934.976318, Training Accuracy=0.92188
Iter 89600, Minibatch Loss= 526.577393, Training Accuracy=0.93750
Iter 90880, Minibatch Loss= 232.094086, Training Accuracy=0.93750
Iter 92160, Minibatch Loss= 292.784729, Training Accuracy=0.92188
Iter 93440, Minibatch Loss= 92.982574, Training Accuracy=0.96875
Iter 94720, Minibatch Loss= 151.433273, Training Accuracy=0.96875
Iter 96000, Minibatch Loss= 235.701874, Training Accuracy=0.95312
Iter 97280, Minibatch Loss= 68.151337, Training Accuracy=0.95312
Iter 98560, Minibatch Loss= 0.000000, Training Accuracy=1.00000
Iter 99840, Minibatch Loss= 538.956787, Training Accuracy=0.90625
Iter 101120, Minibatch Loss= 241.900620, Training Accuracy=0.93750
Iter 102400, Minibatch Loss= 523.403503, Training Accuracy=0.89062
Iter 103680, Minibatch Loss= 302.764954, Training Accuracy=0.93750
Iter 104960, Minibatch Loss= 119.309921, Training Accuracy=0.96875
Iter 106240, Minibatch Loss= 119.943283, Training Accuracy=0.96875
Iter 107520, Minibatch Loss= 45.558273, Training Accuracy=0.96875
Iter 108800, Minibatch Loss= 639.549072, Training Accuracy=0.90625
Iter 110080, Minibatch Loss= 360.823425, Training Accuracy=0.96875
Iter 111360, Minibatch Loss= 656.804932, Training Accuracy=0.85938
Iter 112640, Minibatch Loss= 62.721642, Training Accuracy=0.98438
Iter 113920, Minibatch Loss= 227.738495, Training Accuracy=0.89062
Iter 115200, Minibatch Loss= 107.202942, Training Accuracy=0.96875
Iter 116480, Minibatch Loss= 54.557892, Training Accuracy=0.98438
Iter 117760, Minibatch Loss= 30.022736, Training Accuracy=0.98438
Iter 119040, Minibatch Loss= 55.753250, Training Accuracy=0.96875
Iter 120320, Minibatch Loss= 25.232239, Training Accuracy=0.96875
Iter 121600, Minibatch Loss= 384.985962, Training Accuracy=0.92188
Iter 122880, Minibatch Loss= 185.594620, Training Accuracy=0.96875
Iter 124160, Minibatch Loss= 43.428780, Training Accuracy=0.95312
Iter 125440, Minibatch Loss= 51.860008, Training Accuracy=0.95312
Iter 126720, Minibatch Loss= 39.459244, Training Accuracy=0.98438
Iter 128000, Minibatch Loss= 84.651009, Training Accuracy=0.95312
Iter 129280, Minibatch Loss= 197.585663, Training Accuracy=0.95312
Iter 130560, Minibatch Loss= 270.246216, Training Accuracy=0.96875
Iter 131840, Minibatch Loss= 312.734131, Training Accuracy=0.95312
Iter 133120, Minibatch Loss= 164.027313, Training Accuracy=0.95312
Iter 134400, Minibatch Loss= 241.785522, Training Accuracy=0.96875
Iter 135680, Minibatch Loss= 178.564590, Training Accuracy=0.93750
Iter 136960, Minibatch Loss= 428.156372, Training Accuracy=0.90625
Iter 138240, Minibatch Loss= 42.419006, Training Accuracy=0.98438
Iter 139520, Minibatch Loss= 533.604614, Training Accuracy=0.89062
Iter 140800, Minibatch Loss= 259.403809, Training Accuracy=0.96875
Iter 142080, Minibatch Loss= 305.836060, Training Accuracy=0.93750
Iter 143360, Minibatch Loss= 115.900551, Training Accuracy=0.96875
Iter 144640, Minibatch Loss= 425.214844, Training Accuracy=0.92188
Iter 145920, Minibatch Loss= 282.168182, Training Accuracy=0.93750
Iter 147200, Minibatch Loss= 261.969757, Training Accuracy=0.93750
Iter 148480, Minibatch Loss= 4.863205, Training Accuracy=0.96875
Iter 149760, Minibatch Loss= 80.427750, Training Accuracy=0.98438
Iter 151040, Minibatch Loss= 144.127945, Training Accuracy=0.92188
Iter 152320, Minibatch Loss= 97.322311, Training Accuracy=0.96875
Iter 153600, Minibatch Loss= 188.072937, Training Accuracy=0.93750
Iter 154880, Minibatch Loss= 201.077789, Training Accuracy=0.96875
Iter 156160, Minibatch Loss= 142.754761, Training Accuracy=0.93750
Iter 157440, Minibatch Loss= 90.676544, Training Accuracy=0.98438
Iter 158720, Minibatch Loss= 67.253052, Training Accuracy=0.95312
Iter 160000, Minibatch Loss= 333.338562, Training Accuracy=0.93750
Iter 161280, Minibatch Loss= 311.756287, Training Accuracy=0.96875
Iter 162560, Minibatch Loss= 390.525940, Training Accuracy=0.92188
Iter 163840, Minibatch Loss= 191.545593, Training Accuracy=0.92188
Iter 165120, Minibatch Loss= 230.724457, Training Accuracy=0.92188
Iter 166400, Minibatch Loss= 92.785446, Training Accuracy=0.95312
Iter 167680, Minibatch Loss= 156.399490, Training Accuracy=0.96875
Iter 168960, Minibatch Loss= 0.000000, Training Accuracy=1.00000
Iter 170240, Minibatch Loss= 285.628937, Training Accuracy=0.90625
Iter 171520, Minibatch Loss= 201.931412, Training Accuracy=0.93750
Iter 172800, Minibatch Loss= 24.056946, Training Accuracy=0.98438
Iter 174080, Minibatch Loss= 30.393211, Training Accuracy=0.96875
Iter 175360, Minibatch Loss= 50.383667, Training Accuracy=0.95312
Iter 176640, Minibatch Loss= 216.085892, Training Accuracy=0.95312
Iter 177920, Minibatch Loss= 146.217896, Training Accuracy=0.93750
Iter 179200, Minibatch Loss= 80.371727, Training Accuracy=0.98438
Iter 180480, Minibatch Loss= 212.199783, Training Accuracy=0.95312
Iter 181760, Minibatch Loss= 285.830017, Training Accuracy=0.93750
Iter 183040, Minibatch Loss= 162.950500, Training Accuracy=0.96875
Iter 184320, Minibatch Loss= 399.868896, Training Accuracy=0.90625
Iter 185600, Minibatch Loss= 162.247040, Training Accuracy=0.93750
Iter 186880, Minibatch Loss= 108.299477, Training Accuracy=0.96875
Iter 188160, Minibatch Loss= 169.978210, Training Accuracy=0.95312
Iter 189440, Minibatch Loss= 68.432297, Training Accuracy=0.95312
Iter 190720, Minibatch Loss= 3.985062, Training Accuracy=0.98438
Iter 192000, Minibatch Loss= 302.531555, Training Accuracy=0.90625
Iter 193280, Minibatch Loss= 55.564789, Training Accuracy=0.96875
Iter 194560, Minibatch Loss= 5.660660, Training Accuracy=0.98438
Iter 195840, Minibatch Loss= 218.991119, Training Accuracy=0.92188
Iter 197120, Minibatch Loss= 111.845589, Training Accuracy=0.96875
Iter 198400, Minibatch Loss= 270.518768, Training Accuracy=0.93750
Iter 199680, Minibatch Loss= 35.617252, Training Accuracy=0.98438
Optimization Finished!
Testing Accuracy: 0.984375
```




